commit 6c99fe4d83bae40b60a4d15c0c7994630395dab2
Author: Remo Steiner <example@email.com>
Date:   Fri Sep 26 13:38:30 2025 +0200

    changes for mindmap v1.0.0 on IsaacLab v2.1.0

diff --git a/.gitignore b/.gitignore
index 954045121e..e83f3d5050 100644
--- a/.gitignore
+++ b/.gitignore
@@ -63,3 +63,6 @@ _build
 
 # Teleop Recorded Dataset
 datasets
+
+# XR files
+/openxr/*
diff --git a/scripts/imitation_learning/isaaclab_mimic/annotate_demos.py b/scripts/imitation_learning/isaaclab_mimic/annotate_demos.py
index b7c85d72ab..30eb73fe23 100644
--- a/scripts/imitation_learning/isaaclab_mimic/annotate_demos.py
+++ b/scripts/imitation_learning/isaaclab_mimic/annotate_demos.py
@@ -71,6 +71,8 @@ from isaaclab.utils import configclass
 from isaaclab.utils.datasets import EpisodeData, HDF5DatasetFileHandler
 
 import isaaclab_tasks  # noqa: F401
+# Add external mindmap tasks
+import mindmap.tasks.task_definitions
 from isaaclab_tasks.utils.parse_cfg import parse_env_cfg
 
 is_paused = False
diff --git a/scripts/imitation_learning/isaaclab_mimic/generate_dataset.py b/scripts/imitation_learning/isaaclab_mimic/generate_dataset.py
index f3dd8fa304..816e7e17be 100644
--- a/scripts/imitation_learning/isaaclab_mimic/generate_dataset.py
+++ b/scripts/imitation_learning/isaaclab_mimic/generate_dataset.py
@@ -74,6 +74,8 @@ from isaaclab_mimic.datagen.generation import env_loop, setup_async_generation,
 from isaaclab_mimic.datagen.utils import get_env_name_from_dataset, setup_output_paths
 
 import isaaclab_tasks  # noqa: F401
+# Add external mindmap tasks
+import mindmap.tasks.task_definitions
 
 
 def main():
diff --git a/scripts/tools/record_demos.py b/scripts/tools/record_demos.py
index e5fdde6b3f..4d52502e52 100644
--- a/scripts/tools/record_demos.py
+++ b/scripts/tools/record_demos.py
@@ -35,6 +35,9 @@ import os
 import time
 import torch
 
+from mindmap.embodiments.humanoid.embodiment import map_to_humanoid_action_space
+from mindmap.tasks.tasks import Tasks
+
 # Isaac Lab AppLauncher
 from isaaclab.app import AppLauncher
 
@@ -103,7 +106,10 @@ from isaaclab.envs.ui import EmptyWindow
 from isaaclab.managers import DatasetExportMode
 
 import isaaclab_tasks  # noqa: F401
+# Add external mindmap tasks
+import mindmap.tasks.task_definitions
 from isaaclab_tasks.utils.parse_cfg import parse_env_cfg
+import isaaclab.utils.math as PoseUtils
 
 
 class RateLimiter:
@@ -140,18 +146,18 @@ class RateLimiter:
 
 
 def pre_process_actions(
-    teleop_data: tuple[np.ndarray, bool] | list[tuple[np.ndarray, np.ndarray, np.ndarray]], num_envs: int, device: str
+    teleop_data: tuple[np.ndarray, bool] | list[tuple[np.ndarray, np.ndarray, np.ndarray]], env: gym.Env
 ) -> torch.Tensor:
     """Convert teleop data to the format expected by the environment action space.
 
     Args:
         teleop_data: Data from the teleoperation device.
-        num_envs: Number of environments.
-        device: Device to create tensors on.
 
     Returns:
         Processed actions as a tensor.
     """
+    device = env.device
+    num_envs = env.num_envs
     # compute actions based on environment
     if "Reach" in args_cli.task:
         delta_pose, gripper_command = teleop_data
@@ -160,20 +166,31 @@ def pre_process_actions(
         # note: reach is the only one that uses a different action space
         # compute actions
         return delta_pose
-    elif "PickPlace-GR1T2" in args_cli.task:
-        (left_wrist_pose, right_wrist_pose, hand_joints) = teleop_data[0]
-        # Reconstruct actions_arms tensor with converted positions and rotations
-        actions = torch.tensor(
-            np.concatenate([
-                left_wrist_pose,  # left ee pose
-                right_wrist_pose,  # right ee pose
-                hand_joints,  # hand joint angles
-            ]),
-            device=device,
-            dtype=torch.float32,
-        ).unsqueeze(0)
-        # Concatenate arm poses and hand joint angles
-        return actions
+    elif "GR1T2" in args_cli.task:
+        teleop_data_tensors = [torch.from_numpy(item).to(device).to(torch.float32) for item in teleop_data[0]]
+        (left_wrist_pose, right_wrist_pose, head_yaw_rad_xr, hand_joints) = teleop_data_tensors
+
+        # The head_yaw_rad_xr is expressed in its own frame and is launched to the initial position set
+        # through XrCfg (which does not reset the anchor frame but only where the xr view is spawned).
+        # This should be aligned with the robot viewing angle.
+        # Thus, we need to align the head_yaw_rad_xr with the robot viewing angle to zero when looking straight ahead.
+        xr_anchor_rot_xyzw = torch.tensor([env.cfg.xr.anchor_rot])
+        xr_anchor_rad_z = PoseUtils.euler_xyz_from_quat(xr_anchor_rot_xyzw)[2].item()
+        head_yaw_rad = head_yaw_rad_xr - xr_anchor_rad_z
+        if head_yaw_rad < -np.pi:
+            head_yaw_rad += 2 * np.pi
+        elif head_yaw_rad >= np.pi:
+            head_yaw_rad -= 2 * np.pi
+        assert head_yaw_rad >= -np.pi and head_yaw_rad < np.pi
+
+        action = map_to_humanoid_action_space(
+            task=Tasks.from_full_task_name(args_cli.task),
+            left_wrist_pose=left_wrist_pose,
+            right_wrist_pose=right_wrist_pose,
+            head_yaw_rad=torch.tensor([head_yaw_rad]),
+            hand_joints=hand_joints,
+        )
+        return action.unsqueeze(0)
     else:
         # resolve gripper command
         delta_pose, gripper_command = teleop_data
@@ -374,11 +391,10 @@ def main():
         while simulation_app.is_running():
             # get data from teleop device
             teleop_data = teleop_interface.advance()
-
             # perform action on environment
             if running_recording_instance:
                 # compute actions based on environment
-                actions = pre_process_actions(teleop_data, env.num_envs, env.device)
+                actions = pre_process_actions(teleop_data, env)
                 obv = env.step(actions)
                 if subtasks is not None:
                     if subtasks == {}:
diff --git a/scripts/tools/replay_demos.py b/scripts/tools/replay_demos.py
index d9c1573f4b..a3bc046107 100644
--- a/scripts/tools/replay_demos.py
+++ b/scripts/tools/replay_demos.py
@@ -69,6 +69,8 @@ if args_cli.enable_pinocchio:
     import isaaclab_tasks.manager_based.manipulation.pick_place  # noqa: F401
 
 import isaaclab_tasks  # noqa: F401
+# Add external mindmap tasks
+import mindmap.tasks.task_definitions
 from isaaclab_tasks.utils.parse_cfg import parse_env_cfg
 
 is_paused = False
diff --git a/source/isaaclab/isaaclab/devices/openxr/retargeters/humanoid/fourier/gr1t2_retargeter.py b/source/isaaclab/isaaclab/devices/openxr/retargeters/humanoid/fourier/gr1t2_retargeter.py
index def0e1a89d..14f59697ae 100644
--- a/source/isaaclab/isaaclab/devices/openxr/retargeters/humanoid/fourier/gr1t2_retargeter.py
+++ b/source/isaaclab/isaaclab/devices/openxr/retargeters/humanoid/fourier/gr1t2_retargeter.py
@@ -6,6 +6,7 @@
 import contextlib
 import numpy as np
 import torch
+import math
 
 import isaaclab.sim as sim_utils
 import isaaclab.utils.math as PoseUtils
@@ -60,6 +61,24 @@ class GR1T2Retargeter(RetargeterBase):
             )
             self._markers = VisualizationMarkers(marker_cfg)
 
+    @staticmethod
+    def get_xr_head_yaw_rad(data: dict) -> np.ndarray:
+        """
+        Extracts the head yaw angle in radians from the OpenXR tracking data.
+
+        Args:
+            data (dict): Dictionary containing OpenXR tracking data, where the head pose is accessed via
+                data[OpenXRDevice.TrackingTarget.HEAD]. The pose is expected to be a 7-element array-like
+                (x, y, z, qw, qx, qy, qz).
+
+        Returns:
+            np.ndarray: The head yaw angle in radians in the OpenXR frame.
+        """
+        head_pose = data[OpenXRDevice.TrackingTarget.HEAD]
+        head_quat = torch.tensor(head_pose[3:], dtype=torch.float32).unsqueeze(0)
+        xr_head_yaw_rad = PoseUtils.euler_xyz_from_quat(head_quat)[2]
+        return np.array([xr_head_yaw_rad.item()])
+
     def retarget(self, data: dict) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
         """Convert hand joint poses to robot end-effector commands.
 
@@ -77,6 +96,9 @@ class GR1T2Retargeter(RetargeterBase):
         left_hand_poses = data[OpenXRDevice.TrackingTarget.HAND_LEFT]
         right_hand_poses = data[OpenXRDevice.TrackingTarget.HAND_RIGHT]
 
+        head_yaw_rad = self.get_xr_head_yaw_rad(data)
+
+        # Extract the left and right wrist poses
         left_wrist = left_hand_poses.get("wrist")
         right_wrist = right_hand_poses.get("wrist")
 
@@ -102,7 +124,7 @@ class GR1T2Retargeter(RetargeterBase):
         right_hand_joints = right_retargeted_hand_joints
         retargeted_hand_joints = left_hand_joints + right_hand_joints
 
-        return left_wrist, self._retarget_abs(right_wrist), retargeted_hand_joints
+        return left_wrist, self._retarget_abs(right_wrist), head_yaw_rad, retargeted_hand_joints
 
     def _retarget_abs(self, wrist: np.ndarray) -> np.ndarray:
         """Handle absolute pose retargeting.
diff --git a/source/isaaclab/setup.py b/source/isaaclab/setup.py
index 230c0e55a1..d50a5898ac 100644
--- a/source/isaaclab/setup.py
+++ b/source/isaaclab/setup.py
@@ -34,7 +34,7 @@ INSTALL_REQUIRES = [
     # image processing
     "transformers",
     "einops",  # needed for transformers, doesn't always auto-install
-    "warp-lang",
+    "warp-lang==1.7.2",
     # make sure this is consistent with isaac sim version
     "pillow==11.0.0",
     # livestream
